# תכנית עבודה וארכיטקטורת פלטים – השוואת עברית מודרנית למקורות

## 1. רציונל ומטרות (Research Goals)
הפרויקט בוחן סטטיסטית את טענתה של פרופ' עידית דורון, לפיה התחביר של העברית המודרנית (MH) ביצע "דילוג היסטורי" ואימץ מחדש מאפיינים מקראיים (BH), תוך זניחת מאפיינים של לשון חז"ל (RH).

### שאלות המחקר והמדדים:
1.  **מורכבות ומאפיינים כלליים:** האם המודרנית מציגה מורכבות ("עומק") הדומה למקרא?
    * *מדדים:* אורך משפט, עושר מילולי, עומק עץ תחבירי (Tree Depth).
2.  **מילות שעבוד (Subordination):** בדיקת "התכנסות" – שימוש במבנה מקראי עם מילים חז"ליות.
    * *מדדים:* שכיחות מילות שעבוד (ש-, אשר, כי, אם, פן).
3.  **סדר מילים (Word Order V1 vs V2):** בדיקת הדומיננטיות של נושא-לפני-פועל (SVO/V2) לעומת פועל-לפני-נושא (VSO/V1).
    * *מדדים:* זיהוי מיקום `root` ביחס ל-`nsubj` בעץ התלויות.
4.  **הבעת שייכות (Possession):** סמיכות (Construct) המקראית מול הצירוף הפרוד "של" החז"לי.
    * *מדדים:* יחס בין תגית `compound:smixut` לבין המילה "של".
5.  **צורות מקור (Gerund vs Infinitive):** הראיה החזקה של דורון – חזרת ה-Gerund (שם פועל נטוי/עם נושא) במודרנית.
    * *מדדים:* זיהוי פועל לא נטוי עם נושא (Gerund) לעומת פועל לא נטוי ללא נושא (Infinitive).
6.  **פרופיל לקסיקלי:** שימוש במילים "מסומנות" (חז"ליות) מול "לא מסומנות" (מקראיות).
    * *מדדים:* ספירת זוגות מילים (עץ/אילן, שמש/חמה) והתפלגות POS.

## 2. תהליך העבודה (Pipeline)
1.  **Data Loading:** טעינת קבצי JSON מנותחים מתיקיית `data/output/`.
    * זיהוי הקורפוס (`Biblical`, `Rabbinic`, `Modern`) לפי שם התיקייה.
2.  **Feature Extraction:** מעבר על כל משפט וחילוץ וקטור פיצ'רים שטוח (לפי השאלות הנ"ל).
3.  **Aggregation:** יצירת טבלאות סיכום לכל קורפוס ותת-קורפוס.
4.  **Modeling:** אימון מסווג (Classifier) להבחנה בין BH ל-RH ובדיקת המודרנית.

## 3. ארכיטקטורת הפלטים (Output Architecture)
המערכת מייצרת פלטים ב-4 שכבות. כל הקבצים יישמרו בתיקיית `results/` (או מיקום מוגדר אחר).

### Layer 1: The Master Table
* **Filename:** `all_sentences_features.csv`
* **Description:** טבלה גולמית, שורה לכל משפט. מכילה את כל הספירות (Counts), אורכים, ואינדיקטורים בינאריים (is_v1, has_gerund).

### Layer 2: Aggregated Statistics (CSV)
קבצים אלו משמשים ליצירת הגרפים בדוח:
1.  `corpus_overview_stats.csv` – ממוצעי אורך, עומק עץ, ועושר לשוני לכל קורפוס.
2.  `word_order_v1_v2_stats.csv` – אחוזי V1 מול V2.
3.  `subordination_words_stats.csv` – שכיחות מנורמלת של מילות שעבוד.
4.  `possession_constructions_stats.csv` – יחס סמיכות מול "של".
5.  `gerund_infinitive_stats.csv` – שכיחות צורות המקור (ל-1000 מילים).
6.  `pos_distribution_stats.csv` – התפלגות חלקי דיבר.
7.  `doron_lexical_pairs_stats.csv` – טבלת השוואה לזוגות מילים ספציפיים.
8.  `corpus_distance_matrix.csv` – מטריצת מרחקים וקטוריים בין הקורפוסים.

### Layer 3: Qualitative Examples (TXT)
* `example_sentences_v1_v2.txt` – דוגמאות למשפטים שזוהו כ-V1/V2.
* `example_sentences_possessive.txt` – דוגמאות למבני שייכות שונים.

### Layer 4: Classification Results (CSV)
* `classifier_metrics_historical.csv` – ביצועי המודל על דאטה היסטורי.
* `classifier_predictions_modern.csv` – כיצד המודל סיווג את הטקסטים המודרניים (אחוז קרבה למקרא/חז"ל).

## 4. הנחיות טכניות למימוש
* הקוד ב-Python יסתמך על שדות ה-JSON של Dicta: `tokens`, `lex`, `morph`, `syntax`.
* זיהוי Gerund דורש בדיקה משולבת של `morph` (חוסר נטייה) ו-`syntax` (קיום `nsubj`).
* זיהוי V1/V2 דורש השוואת אינדקסים של `root_idx` ו-`nsubj`.

